# Plan Step 04 — Tools

## Goal
Implement all tools used by agent nodes. Each tool is standalone.
`base.py` provides `get_tools()`, `get_tools_by_name()`, and `make_tool_node()` for
consistent tool binding and execution across all nodes.

---

## 4.1 Tool Inventory

| Tool Name | File | Purpose | Used By |
|---|---|---|---|
| `semantic_search` | `rag_tool.py` | Pure vector search (BGE-M3 cosine similarity) | context_loader, planner, task agents, writer agents |
| `hybrid_search` | `rag_tool.py` | RRF fusion of vector + BM25 FTS — **default** | same as above |
| `web_search_tool` | `web_search_tool.py` | Tavily web search | task agents, research supervisor |
| `memory_search_tool` | `memory_tools.py` | Query user memory namespaces | supervisors, planner |
| `write_memory_tool` | `memory_tools.py` | Trigger LLM memory update | supervisors (via memory_writer node) |
| `Question` | `question_tool.py` | Signal tool to ask user a question | supervisors (triggers HITL) |
| `Done` | `question_tool.py` | Signal tool to indicate completion | task agents, writer agents |

**Search strategy:** the agent LLM reads both tool docstrings and decides which to call.
It can issue **multiple queries per turn** (different phrasings or sub-questions) by calling
the tool repeatedly. Default to `hybrid_search`; `semantic_search` is for broad conceptual
queries where exact word matches don't matter.

---

## 4.2 File: `src/strategic_analyst/tools/rag_tool.py`

### Embedding helper
Query embeddings are generated by calling the OVH BGE-M3 endpoint via `aiohttp`.
No local model download is required — the same `OVH_KEY` token used for LLM calls
authenticates the embedding API.

```python
import os
import asyncio
from functools import lru_cache

import aiohttp
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from supabase import create_client, Client


# ── OVH Embedding API ─────────────────────────────────────────────────────────

_OVH_EMBEDDING_URL_DEFAULT = "https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec"


def _get_ovh_token() -> str:
    token = os.getenv("OVH_KEY") or os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN")
    if not token:
        raise EnvironmentError("OVH auth token not found. Set OVH_KEY in your .env file.")
    return token


async def _embed_query(text: str) -> list[float]:
    """
    Call the OVH BGE-M3 endpoint and return a 1024-dim dense vector.

    POST OVH_EMBEDDING_ENDPOINT_URL
    Authorization: Bearer <OVH_KEY>
    Body: {"inputs": "<text>"}
    Response: [[float, ...]]  (HuggingFace inference API — outer list = batch dim)
    """
    url   = os.getenv("OVH_EMBEDDING_ENDPOINT_URL", _OVH_EMBEDDING_URL_DEFAULT)
    token = _get_ovh_token()

    async with aiohttp.ClientSession() as session:
        async with session.post(
            url,
            headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json"},
            json={"inputs": text},
        ) as resp:
            resp.raise_for_status()
            result = await resp.json()

    # Normalise: [[float,...]] -> result[0],  [float,...] -> result
    if isinstance(result, list) and len(result) > 0 and isinstance(result[0], list):
        return result[0]
    return result


# ── Supabase client (cached sync client, called via executor) ─────────────────

@lru_cache(maxsize=1)
def _get_supabase() -> Client:
    return create_client(
        os.getenv("SUPABASE_URL"),
        os.getenv("SUPABASE_SERVICE_KEY"),
    )


async def _call_rpc(func_name: str, params: dict) -> list[dict]:
    """Execute a Supabase RPC call in a thread executor."""
    client = _get_supabase()
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(
        None,
        lambda: client.rpc(func_name, params).execute(),
    )
    return response.data or []


# ── Result formatter ──────────────────────────────────────────────────────────

def _format_results(rows: list[dict], score_label: str = "Relevance") -> str:
    if not rows:
        return "No relevant documents found in the knowledge base."
    parts = []
    for i, row in enumerate(rows, 1):
        heading = f" — {row['heading']}" if row.get("heading") else ""
        page    = f" (p.{row['page_number']})" if row.get("page_number") else ""
        score   = row.get("score", 0)
        parts.append(
            f"[{i}] {row['document_title']}{heading}{page} | {score_label}: {score:.4f}\n"
            f"{row['content']}"
        )
    return "\n\n---\n\n".join(parts)


# ── Tool 1: Semantic Search ───────────────────────────────────────────────────

class SemanticSearchInput(BaseModel):
    query_text: str = Field(description="Natural language question or topic to search for")
    limit: int = Field(default=10, description="Number of results to return (1-20)")


@tool(args_schema=SemanticSearchInput)
async def semantic_search(query_text: str, limit: int = 10) -> str:
    """
    Search the strategic knowledge base using vector similarity (BGE-M3, cosine distance).

    Best for:
    - Broad conceptual questions ("what is our competitive position?")
    - Paraphrased queries where the exact words don't appear in the source
    - Finding thematically related content across different phrasings

    Prefer hybrid_search for specific company names, financial figures, metrics,
    or any query where exact keyword matches matter.

    You can call this tool multiple times with different phrasings of the same question
    to improve retrieval coverage.

    Always cite document_title and page_number in your response.
    """
    embedding = await _embed_query(query_text)
    rows = await _call_rpc(
        "semantic_search",
        {"query_embedding": embedding, "match_count": min(limit, 20)},
    )
    return _format_results(rows, score_label="Similarity")


# ── Tool 2: Hybrid Search (DEFAULT) ──────────────────────────────────────────

class HybridSearchInput(BaseModel):
    query_text: str = Field(description="Natural language question or search terms")
    limit: int = Field(default=10, description="Number of results to return (1-20)")


@tool(args_schema=HybridSearchInput)
async def hybrid_search(query_text: str, limit: int = 10) -> str:
    """
    Search the strategic knowledge base using vector similarity AND BM25 keyword
    matching, fused via Reciprocal Rank Fusion (RRF).

    DEFAULT SEARCH — use this for most queries, especially:
    - Specific company names, product names, brand terms
    - Financial figures, percentages, dates, metrics
    - Technical or industry-specific terminology
    - Queries where exact keyword matches matter alongside semantic meaning

    You can call this tool multiple times with different phrasings, or break a
    complex question into focused sub-queries for better coverage.

    Always cite document_title and page_number in your response.
    """
    embedding = await _embed_query(query_text)
    rows = await _call_rpc(
        "hybrid_search",
        {
            "query_embedding": embedding,
            "query_text": query_text,
            "match_count": min(limit, 20),
        },
    )
    return _format_results(rows, score_label="RRF Score")
```

---

## 4.3 File: `src/strategic_analyst/tools/web_search_tool.py`

```python
import os
import asyncio
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from tavily import TavilyClient


class WebSearchInput(BaseModel):
    query: str = Field(description="The web search query")
    max_results: int = Field(default=5, description="Maximum number of search results to return")
    search_depth: str = Field(
        default="advanced",
        description="Search depth: 'basic' for quick results, 'advanced' for deeper analysis"
    )


@tool(args_schema=WebSearchInput)
async def web_search_tool(
    query: str,
    max_results: int = 5,
    search_depth: str = "advanced",
) -> str:
    """
    Search the web for current market intelligence, competitor information,
    industry trends, news, and publicly available strategic data.

    Use this when:
    - The company knowledge base lacks sufficient or up-to-date information
    - You need current events, recent news, or real-time market data
    - You want to validate internal findings against external sources

    Returns formatted search results with title, URL, and content excerpt.
    Always cite the URL in your response.
    """
    client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(
        None,
        lambda: client.search(
            query=query,
            max_results=max_results,
            search_depth=search_depth,
            include_answer=True,
        ),
    )

    parts = []
    if response.get("answer"):
        parts.append(f"**Quick Answer:** {response['answer']}\n")
    for i, result in enumerate(response.get("results", []), 1):
        parts.append(
            f"[{i}] **{result.get('title', 'Untitled')}**\n"
            f"URL: {result.get('url', 'No URL')}\n"
            f"{result.get('content', 'No content')}\n"
        )
    return "\n---\n".join(parts) if parts else "No web search results found."
```

---

## 4.4 File: `src/strategic_analyst/tools/memory_tools.py`

Memory tools are **stubs** — the LLM sees their docstrings and schemas for tool selection,
but actual execution is injected by `make_tool_node` in `base.py` (which has `store` +
`user_id` in scope).

```python
from typing import Literal
from langchain_core.tools import tool
from pydantic import BaseModel, Field


class MemorySearchInput(BaseModel):
    namespace_type: Literal[
        "user_profile", "company_profile", "user_preferences", "episodic_memory"
    ] = Field(description="Which memory namespace to search")
    query: str = Field(
        description="What you are looking for in this memory namespace"
    )


@tool(args_schema=MemorySearchInput)
def memory_search_tool(namespace_type: str, query: str) -> str:
    """
    Search the user's personal memory for relevant context.

    Namespaces:
    - user_profile:      who the user is, their role, communication style
    - company_profile:   company context, industry, competitors, vocabulary
    - user_preferences:  preferred report format, frameworks, verbosity, citation style
    - episodic_memory:   past research sessions, important dates, temporal notes

    Always check memory before making assumptions about the user or their company.
    """
    raise NotImplementedError(
        "memory_search_tool must be invoked via the node-level wrapper that injects store + user_id"
    )


class WriteMemoryInput(BaseModel):
    namespace_type: Literal[
        "user_profile", "company_profile", "user_preferences", "episodic_memory"
    ] = Field(description="Which memory namespace to update")
    update_reason: str = Field(
        description="Explain what new information was discovered and why this memory should be updated"
    )
    context: str = Field(
        description="The new information or feedback that should be incorporated into memory"
    )


@tool(args_schema=WriteMemoryInput)
def write_memory_tool(namespace_type: str, update_reason: str, context: str) -> str:
    """
    Update a user memory namespace with new information discovered during research
    or revealed through user feedback.

    Use this when:
    - User reveals personal information (-> user_profile)
    - Research uncovers new company context (-> company_profile)
    - User gives style or format feedback (-> user_preferences)
    - A research session concludes with notable temporal findings (-> episodic_memory)

    IMPORTANT: This makes a targeted LLM-driven update — it never overwrites existing memory.
    """
    raise NotImplementedError(
        "write_memory_tool must be invoked via the node-level wrapper that injects store + user_id"
    )
```

---

## 4.5 File: `src/strategic_analyst/tools/question_tool.py`

Signal tools used by supervisors and task agents to communicate control flow.

```python
from langchain_core.tools import tool
from pydantic import BaseModel, Field


@tool
class Question(BaseModel):
    """
    Ask the user a clarifying question or request their input before proceeding.

    Use this when:
    - You need additional context to complete the research task
    - You found something unexpected and want the user to decide whether to pursue it
    - You are presenting discoveries and want to confirm direction before writing the report
    - You need to verify an assumption before committing to a report section

    The question will be shown to the user via the human-in-the-loop interface.
    """
    content: str = Field(description="The question to ask the user")


@tool
class Done(BaseModel):
    """
    Signal that the current task or subtask is complete.

    Use this after:
    - Completing all assigned research tasks
    - Writing a final report section
    - Finishing a response to a user follow-up

    Do NOT use this if you still have pending tool calls to make.
    """
    done: bool = Field(default=True, description="Always True — signals completion")
    summary: str = Field(
        default="",
        description="Brief summary of what was accomplished in this task",
    )
```

---

## 4.6 File: `src/strategic_analyst/tools/base.py`

```python
from typing import Optional, List, Dict
from langchain_core.tools import BaseTool
from langgraph.store.base import BaseStore

from strategic_analyst.tools.rag_tool import semantic_search, hybrid_search
from strategic_analyst.tools.web_search_tool import web_search_tool
from strategic_analyst.tools.memory_tools import memory_search_tool, write_memory_tool
from strategic_analyst.tools.question_tool import Question, Done


# ── Tool Sets Per Agent Role ───────────────────────────────────────────────────

MAIN_AGENT_TOOLS          = ["semantic_search", "hybrid_search", "memory_search_tool"]
PLANNER_TOOLS             = ["semantic_search", "hybrid_search", "memory_search_tool"]
RESEARCH_SUPERVISOR_TOOLS = ["semantic_search", "hybrid_search", "web_search_tool",
                              "memory_search_tool", "write_memory_tool", "Question"]
TASK_AGENT_TOOLS          = ["semantic_search", "hybrid_search", "web_search_tool", "Done"]
REPORT_SUPERVISOR_TOOLS   = ["memory_search_tool", "write_memory_tool", "Question"]
REPORT_WRITER_TOOLS       = ["semantic_search", "hybrid_search", "memory_search_tool", "Done"]

ALL_TOOLS_REGISTRY: Dict[str, BaseTool] = {
    "semantic_search":    semantic_search,
    "hybrid_search":      hybrid_search,
    "web_search_tool":    web_search_tool,
    "memory_search_tool": memory_search_tool,
    "write_memory_tool":  write_memory_tool,
    "Question":           Question,
    "Done":               Done,
}


def get_tools(tool_names: Optional[List[str]] = None) -> List[BaseTool]:
    """Return tool objects by name. Returns all registered tools if tool_names is None."""
    if tool_names is None:
        return list(ALL_TOOLS_REGISTRY.values())
    return [ALL_TOOLS_REGISTRY[name] for name in tool_names if name in ALL_TOOLS_REGISTRY]


def get_tools_by_name(tools: List[BaseTool]) -> Dict[str, BaseTool]:
    """Convert a list of tools to a name-keyed dict for fast lookup."""
    return {t.name: t for t in tools}


def make_tool_node(tools: List[BaseTool], store: BaseStore, user_id: str):
    """
    Create an async tool execution node with store injection for memory tools.

    - memory_search_tool  -> calls search_memory(store, user_id, ...)
    - write_memory_tool   -> calls update_memory_with_llm_async(store, ...)
    - Question / Done     -> signal acknowledgement (no side effects)
    - all other tools     -> direct ainvoke
    """
    tools_by_name = get_tools_by_name(tools)

    from strategic_analyst.memory import (
        search_memory,
        update_memory_with_llm_async,
        user_profile_ns,
        company_profile_ns,
        user_preferences_ns,
        episodic_memory_ns,
        NAMESPACE_KEYS,
    )

    NAMESPACE_MAP = {
        "user_profile":    (user_profile_ns(user_id),    NAMESPACE_KEYS["user_profile"]),
        "company_profile": (company_profile_ns(user_id), NAMESPACE_KEYS["company_profile"]),
        "user_preferences":(user_preferences_ns(user_id),NAMESPACE_KEYS["user_preferences"]),
        "episodic_memory": (episodic_memory_ns(user_id), NAMESPACE_KEYS["episodic_memory"]),
    }

    async def tool_node(state: dict) -> dict:
        results = []
        for tool_call in state["messages"][-1].tool_calls:
            tool_name = tool_call["name"]
            args      = tool_call["args"]
            call_id   = tool_call["id"]

            if tool_name == "memory_search_tool":
                ns_type = args.get("namespace_type", "user_profile")
                content = search_memory(store, user_id, ns_type, args.get("query", ""))
                results.append({"role": "tool", "content": content, "tool_call_id": call_id})

            elif tool_name == "write_memory_tool":
                ns_type = args.get("namespace_type", "user_preferences")
                namespace, key = NAMESPACE_MAP[ns_type]
                await update_memory_with_llm_async(
                    store=store,
                    namespace=namespace,
                    key=key,
                    context_messages=[{"role": "user", "content": args.get("context", "")}],
                    update_reason=args.get("update_reason", "Agent-initiated memory update"),
                )
                results.append({
                    "role": "tool",
                    "content": f"Memory namespace '{ns_type}' updated successfully.",
                    "tool_call_id": call_id,
                })

            elif tool_name in ("Question", "Done"):
                results.append({
                    "role": "tool",
                    "content": f"Signal '{tool_name}' received.",
                    "tool_call_id": call_id,
                })

            else:
                tool_obj = tools_by_name[tool_name]
                observation = await tool_obj.ainvoke(args)
                results.append({
                    "role": "tool",
                    "content": str(observation),
                    "tool_call_id": call_id,
                })

        return {"messages": results}

    return tool_node
```

---

## 4.7 LLM Instantiation Pattern (reference for nodes)

All nodes use `ChatOpenAI` pointed at OVH AI Endpoints:

```python
from langchain_openai import ChatOpenAI
import os

def get_llm(model_env_var: str = "MAIN_AGENT_MODEL") -> ChatOpenAI:
    return ChatOpenAI(
        model=os.getenv(model_env_var, "Mistral-Nemo-Instruct-2407"),
        api_key=os.getenv("OVH_KEY") or os.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"),
        base_url=os.getenv("OVH_API_BASE_URL", "https://oai.endpoints.kepler.ai.cloud.ovh.net/v1"),
        temperature=0.0,
    )

# Per-role examples:
llm_main    = get_llm("MAIN_AGENT_MODEL")   # Mistral-Nemo for planning/supervision
llm_utility = get_llm("UTILITY_MODEL")      # Mistral-Nemo for memory updates
llm_writer  = get_llm("WRITER_MODEL")       # gpt-oss-20b for long report generation
```

---

## Completion Checklist

- [ ] `rag_tool.py` — `semantic_search` + `hybrid_search` with BGE-M3 embedding
- [ ] `web_search_tool.py` — Tavily async wrapper
- [ ] `memory_tools.py` — stub tools with correct schemas
- [ ] `question_tool.py` — `Question` and `Done` signal tools
- [ ] `base.py` — registry, tool sets, `make_tool_node` factory
- [ ] `tools/__init__.py` — all exports
- [ ] Verify: `uv run python -c "from strategic_analyst.tools.base import get_tools, TASK_AGENT_TOOLS; print(get_tools(TASK_AGENT_TOOLS))"`
